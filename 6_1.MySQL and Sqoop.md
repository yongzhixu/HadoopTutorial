[TOC]

### Connect to mysql

mysql -u root -p hadoop
#### Note on HDP 2.6.5 and later!

On newer HDP sandboxes, mysql's root account had no default password. You need to set one up the hard way first. If the password hadoop doesn't work, enter the following commands:

```sh
su root
systemctl stop mysqld
systemctl set-environment MYSQLD_OPTS="--skip-grant-tables  --skip-networking"
systemctl start mysqld
mysql -uroot
```

Then, in the SQL shell, run:

```sh
FLUSH PRIVILEGES;
ALTER USER 'root'@'localhost' IDENTIFIED BY 'hadoop'
FLUSH PRIVILEGES;
QUIT;
```

Back at the shell runï¼›

```sh
systemctl unset-environment MYSQLD_OPTS
systemctl restart mysqld
exit
```

Now, you should be able to successfully connect with:

```sh
mysql -uroot -phadoop
```

### Import MovieLens data into a MySQL database

Create Databse

```
create database movielens;
show database;
```

Download Data SQL file

```sh
wget http://media.sundog-soft.com/hadoop/movielens.sql
```

Load data

```sql
SET NAMES 'utf8';
SET CHARACTER SET utf8;
use movielens;
source movielens.sql;
show tables;
select * from movies limit 10;
```

Create anonymous user 

- this way you don't have to specify user or password while manipulate data under localhost. 

```sql
CREATE USER ''@'localhost';
GRANT ALL ON movielens.* TO ''@'localhost';
```

exit mysql shell. 

### Import the movies to HDFS

```sql
sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies -m 1
```

Or Specify the user and password (mysql user)

```sql
sqoop import --username root -P --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies -m 1"
```

```sql
-m 1: only use only mapper to import the data. 
```

- double check data imported 

```sh
hadoop fs -ls /user/maria_dev/movies
```

- delete folder

```sh
hadoop fs -rm -r /user/maria_dev/movies
```

### Import the movies into Hive

- import into the default database and save to the default dir

```sql
sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies -m 1 --hive-import
```
- specify saved dir, hive-table, mysql user
```sql
-- mysql shell
CREATE USER 'scimport'@'localhost' IDENTIFIED BY 'hadoop';
GRANT ALL PRIVILEGES ON movielens.* to 'scimport'@'localhost';
-- exit
sqoop import --username scimport --password hadoop --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies -m 1  --hive-import --hive-database foodmart --create-hive-table --hive-table facmovies --target-dir /tmp/customer/facmovies
```
```sql

```



### Export the movies back into MySQL

#### Create MySQL table

- Target table must already exist in MySQL, with columns in expected order

```SQL
create tbale export_movies (id INTEGER, title VARCHAR(255), release_deta DATE);
```

#### Eexport from cluster to Mysql

```sql
sqoop export --connect jdbc:mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver--table export_movies --export-dir/apps/hive/warehouse/movies --input-fields-terminated-by '\0001';

sqoop export --connect jdbc:mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table export_movies --export-dir /apps/hive/warehouse/foodmart.db/facmovies --input-fields-terminated-by '\0001';
```

<!--/apps/hive/warehouse/movies, notes while hive table store in HDFS, in other system, it might be /user/hive/warehouse/. or any target dir specified while import data from other sources into hive-->



### Bug fix

#### User null does not belong to hadoop

```sh
What solved it for me was to add the user to the hadoop group. 
so switch user to root and then usermod -a -G hadoop <user> and that worked for me
usermod -a -G hadoop root
usermod -a -G hadoop maria_dev
```

